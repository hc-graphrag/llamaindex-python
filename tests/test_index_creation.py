#!/usr/bin/env python3
"""
æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

ã“ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã¯ä»¥ä¸‹ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ï¼š
1. åŸºæœ¬çš„ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆæ©Ÿèƒ½
2. è¤‡æ•°ã‚¿ã‚¤ãƒ—ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢çµ±åˆ
3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å›å¾©æ©Ÿèƒ½
4. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
"""

import os
import tempfile
import shutil
import yaml
import pandas as pd
from pathlib import Path
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

# å¿…è¦ãªç’°å¢ƒå¤‰æ•°è¨­å®š
os.environ["ANTHROPIC_API_KEY"] = "test-key"

try:
    from graphrag_anthropic_llamaindex.document_processor import add_documents
    from graphrag_anthropic_llamaindex.vector_store_manager import get_vector_store, get_index
    from graphrag_anthropic_llamaindex.search_processor import search_index
    from graphrag_anthropic_llamaindex.config_manager import load_config
    from graphrag_anthropic_llamaindex.file_filter import FileFilter
    from llama_index.core import Settings
    from llama_index.llms.anthropic import Anthropic
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.core.node_parser import SentenceSplitter
except ImportError as e:
    print(f"âŒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    sys.exit(1)

class TestIndexCreation:
    """æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def setup_method(self):
        """å„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè¡Œå‰ã®åˆæœŸåŒ–"""
        self.test_dir = tempfile.mkdtemp(prefix="test_graphrag_")
        self.config = self._create_test_config()
        self.test_data_dir = os.path.join(self.test_dir, "test_data")
        self.output_dir = os.path.join(self.test_dir, "output")
        os.makedirs(self.test_data_dir, exist_ok=True)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # LlamaIndex SettingsåˆæœŸåŒ–
        self._setup_llama_index_settings()
        
    def teardown_method(self):
        """å„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè¡Œå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir, ignore_errors=True)
    
    def _create_test_config(self):
        """ãƒ†ã‚¹ãƒˆç”¨è¨­å®šã‚’ä½œæˆ"""
        return {
            "anthropic": {
                "api_key": "test-key",
                "model": "claude-3-haiku-20240307"
            },
            "embedding_model": {
                "name": "intfloat/multilingual-e5-small"
            },
            "chunking": {
                "chunk_size": 512,
                "chunk_overlap": 50
            },
            "input_dir": self.test_data_dir if hasattr(self, 'test_data_dir') else "./test_data",
            "output_dir": self.output_dir if hasattr(self, 'output_dir') else "./test_output",
            "vector_store": {
                "type": "lancedb",
                "lancedb": {
                    "uri": "test_lancedb_main",
                    "table_name": "vectors"
                }
            },
            "entity_vector_store": {
                "type": "lancedb", 
                "lancedb": {
                    "uri": "test_lancedb_entities",
                    "table_name": "entities_vectors"
                }
            },
            "community_vector_store": {
                "type": "lancedb",
                "lancedb": {
                    "uri": "test_lancedb_communities", 
                    "table_name": "community_vectors"
                }
            },
            "community_detection": {
                "max_cluster_size": 5,
                "use_lcc": True,
                "seed": 42
            }
        }
    
    def _create_test_data(self):
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        test_files = {
            "document1.txt": "Python is a high-level programming language. It supports object-oriented programming.",
            "document2.txt": "Machine learning uses algorithms to analyze data patterns. Neural networks are powerful tools.",
            "document3.txt": "GraphRAG combines graph databases with retrieval-augmented generation for better search.",
            "data.csv": "name,description,category\nPython,Programming Language,Software\nAI,Artificial Intelligence,Technology\nGraph,Data Structure,Computer Science"
        }
        
        for filename, content in test_files.items():
            file_path = os.path.join(self.test_data_dir, filename)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        
        return list(test_files.keys())
    
    def _setup_llama_index_settings(self):
        """LlamaIndex SettingsåˆæœŸåŒ–"""
        try:
            # ãƒ¢ãƒƒã‚¯LLMè¨­å®šï¼ˆå®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
            from llama_index.core.llms.mock import MockLLM
            
            # ãƒ¢ãƒƒã‚¯LLMã§ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºã®JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
            mock_llm = MockLLM(max_tokens=1000)
            mock_llm._response = """[START_JSON]
{
    "entities": [
        {"name": "Python", "type": "Programming Language"},
        {"name": "Machine Learning", "type": "Technology"},
        {"name": "GraphRAG", "type": "Technology"}
    ],
    "relationships": [
        {"source": "Python", "target": "Machine Learning", "type": "supports", "description": "Python supports machine learning"},
        {"source": "GraphRAG", "target": "Machine Learning", "type": "uses", "description": "GraphRAG uses machine learning techniques"}
    ]
}
[END_JSON]"""
            
            Settings.llm = mock_llm
            
            # Embedding modelè¨­å®š
            embed_model = HuggingFaceEmbedding(model_name=self.config["embedding_model"]["name"])
            Settings.embed_model = embed_model
            
            # Node parserè¨­å®š
            node_parser = SentenceSplitter(
                chunk_size=self.config["chunking"]["chunk_size"],
                chunk_overlap=self.config["chunking"]["chunk_overlap"]
            )
            Settings.node_parser = node_parser
            
            print(f"âœ… LlamaIndex Settingsè¨­å®šå®Œäº†: LLM={type(Settings.llm).__name__}, Embed={type(Settings.embed_model).__name__}")
            
        except Exception as e:
            print(f"âŒ LlamaIndex Settingsè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã™ã‚‹ã“ã¨ã‚’æ˜ç¤ºã™ã‚‹ãŸã‚ã«ä¾‹å¤–ã‚’å†ç™ºç”Ÿ
            raise

class TestBasicIndexCreation(TestIndexCreation):
    """åŸºæœ¬çš„ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãƒ†ã‚¹ãƒˆ"""
    
    def test_basic_document_processing(self):
        """åŸºæœ¬çš„ãªæ–‡æ›¸å‡¦ç†ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ§ª ãƒ†ã‚¹ãƒˆ: åŸºæœ¬çš„ãªæ–‡æ›¸å‡¦ç†ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        test_files = self._create_test_data()
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {test_files}")
        
        try:
            # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢è¨­å®š
            main_vector_store = get_vector_store(self.config, store_type="main")
            entity_vector_store = get_vector_store(self.config, store_type="entity")
            community_vector_store = get_vector_store(self.config, store_type="community")
            
            print(f"ğŸ”§ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢è¨­å®šå®Œäº†")
            print(f"   - ãƒ¡ã‚¤ãƒ³: {type(main_vector_store).__name__ if main_vector_store else 'None'}")
            print(f"   - ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£: {type(entity_vector_store).__name__ if entity_vector_store else 'None'}")
            print(f"   - ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£: {type(community_vector_store).__name__ if community_vector_store else 'None'}")
            
            # æ–‡æ›¸å‡¦ç†å®Ÿè¡Œ
            community_detection_config = self.config.get("community_detection", {})
            file_filter = FileFilter()
            
            print(f"ğŸ“ æ–‡æ›¸å‡¦ç†é–‹å§‹...")
            add_documents(
                input_dir=self.test_data_dir,
                output_dir=self.output_dir,
                vector_store=main_vector_store,
                entity_vector_store=entity_vector_store,
                community_vector_store=community_vector_store,
                community_detection_config=community_detection_config,
                use_archive_reader=False,
                file_filter=file_filter
            )
            
            print(f"âœ… æ–‡æ›¸å‡¦ç†å®Œäº†")
            
            # çµæœæ¤œè¨¼
            self._verify_processing_results()
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
    
    def test_vector_store_creation(self):
        """ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆã®ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ§ª ãƒ†ã‚¹ãƒˆ: ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆ")
        
        # å„ã‚¿ã‚¤ãƒ—ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆãƒ†ã‚¹ãƒˆ
        store_types = ["main", "entity", "community"]
        
        for store_type in store_types:
            print(f"ğŸ“Š {store_type}ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆãƒ†ã‚¹ãƒˆ")
            
            vector_store = get_vector_store(self.config, store_type=store_type)
            
            if vector_store is not None:
                print(f"   âœ… {store_type}: {type(vector_store).__name__}")
                
                # LanceDBã®å ´åˆã€è¨­å®šç¢ºèª
                if hasattr(vector_store, '_uri'):
                    print(f"      URI: {vector_store._uri}")
                if hasattr(vector_store, '_table_name'):
                    print(f"      ãƒ†ãƒ¼ãƒ–ãƒ«å: {vector_store._table_name}")
            else:
                print(f"   âŒ {store_type}: ä½œæˆå¤±æ•—")
                
    def _verify_processing_results(self):
        """å‡¦ç†çµæœã®æ¤œè¨¼"""
        print("ğŸ” å‡¦ç†çµæœæ¤œè¨¼é–‹å§‹")
        
        # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        expected_files = [
            "processed_files.parquet",
            "entities.parquet", 
            "relationships.parquet",
            "communities.parquet",
            "community_summaries.parquet"
        ]
        
        for filename in expected_files:
            file_path = os.path.join(self.output_dir, filename)
            if os.path.exists(file_path):
                file_size = os.path.getsize(file_path)
                print(f"   âœ… {filename}: {file_size} bytes")
            else:
                print(f"   âŒ {filename}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
        vector_dirs = [
            "test_lancedb_main",
            "test_lancedb_entities", 
            "test_lancedb_communities"
        ]
        
        for dir_name in vector_dirs:
            dir_path = os.path.join(self.output_dir, dir_name)
            if os.path.exists(dir_path):
                files = os.listdir(dir_path)
                print(f"   âœ… {dir_name}: {len(files)} ãƒ•ã‚¡ã‚¤ãƒ«")
            else:
                print(f"   âŒ {dir_name}: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

class TestIndexIntegrity(TestIndexCreation):
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ"""
    
    def test_data_consistency(self):
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ§ª ãƒ†ã‚¹ãƒˆ: ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆã¨å‡¦ç†
        self._create_test_data()
        self._process_test_documents()
        
        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª
        self._check_data_relationships()
    
    def _process_test_documents(self):
        """ãƒ†ã‚¹ãƒˆæ–‡æ›¸ã®å‡¦ç†"""
        main_vector_store = get_vector_store(self.config, store_type="main")
        entity_vector_store = get_vector_store(self.config, store_type="entity")
        community_vector_store = get_vector_store(self.config, store_type="community")
        
        add_documents(
            input_dir=self.test_data_dir,
            output_dir=self.output_dir,
            vector_store=main_vector_store,
            entity_vector_store=entity_vector_store,
            community_vector_store=community_vector_store,
            community_detection_config=self.config.get("community_detection", {}),
            use_archive_reader=False,
            file_filter=FileFilter()
        )
    
    def _check_data_relationships(self):
        """ãƒ‡ãƒ¼ã‚¿ã®é–¢ä¿‚æ€§ç¢ºèª"""
        try:
            # Parquetãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            entities_df = pd.read_parquet(os.path.join(self.output_dir, "entities.parquet"))
            relationships_df = pd.read_parquet(os.path.join(self.output_dir, "relationships.parquet"))
            
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
            print(f"   - ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°: {len(entities_df)}")
            print(f"   - é–¢ä¿‚æ€§æ•°: {len(relationships_df)}")
            
            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢ä¿‚æ€§ã®æ•´åˆæ€§ç¢ºèª
            entity_names = set(entities_df['name'].tolist())
            relation_entities = set()
            
            for _, row in relationships_df.iterrows():
                relation_entities.add(row['source'])
                relation_entities.add(row['target'])
            
            missing_entities = relation_entities - entity_names
            if missing_entities:
                print(f"   âŒ é–¢ä¿‚æ€§ã«å­˜åœ¨ã™ã‚‹ãŒã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ†ãƒ¼ãƒ–ãƒ«ã«ç„¡ã„ã‚‚ã®: {missing_entities}")
            else:
                print(f"   âœ… ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢ä¿‚æ€§ã®æ•´åˆæ€§: OK")
                
        except Exception as e:
            print(f"   âŒ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}")

def run_comprehensive_test():
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("ğŸš€ æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆé–‹å§‹\n")
    
    # ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    basic_test = TestBasicIndexCreation()
    integrity_test = TestIndexIntegrity()
    
    test_results = {
        "passed": 0,
        "failed": 0,
        "errors": []
    }
    
    tests = [
        ("åŸºæœ¬çš„ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ", basic_test.test_basic_document_processing),
        ("ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆ", basic_test.test_vector_store_creation),
        ("ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§", integrity_test.test_data_consistency)
    ]
    
    for test_name, test_method in tests:
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ§ª å®Ÿè¡Œä¸­: {test_name}")
            print(f"{'='*60}")
            
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            if hasattr(test_method, '__self__'):
                test_method.__self__.setup_method()
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_method()
            
            print(f"âœ… {test_name}: æˆåŠŸ")
            test_results["passed"] += 1
            
        except Exception as e:
            print(f"âŒ {test_name}: å¤±æ•— - {str(e)}")
            test_results["failed"] += 1
            test_results["errors"].append(f"{test_name}: {str(e)}")
            
        finally:
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if hasattr(test_method, '__self__'):
                test_method.__self__.teardown_method()
    
    # ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    print(f"âœ… æˆåŠŸ: {test_results['passed']}")
    print(f"âŒ å¤±æ•—: {test_results['failed']}")
    print(f"ğŸ“ ç·ãƒ†ã‚¹ãƒˆæ•°: {test_results['passed'] + test_results['failed']}")
    
    if test_results["errors"]:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼è©³ç´°:")
        for error in test_results["errors"]:
            print(f"   - {error}")
    
    return test_results

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = run_comprehensive_test()
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰è¨­å®š
    exit_code = 0 if results["failed"] == 0 else 1
    sys.exit(exit_code)