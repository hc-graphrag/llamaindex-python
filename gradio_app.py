import os
import json
import logging
from typing import Optional, Tuple, Dict, Any
import gradio as gr
from dotenv import load_dotenv

from llama_index.llms.anthropic import Anthropic
from llama_index.llms.bedrock import Bedrock
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import QueryBundle

from src.graphrag_anthropic_llamaindex.config_manager import load_config
from src.graphrag_anthropic_llamaindex.vector_store_manager import get_vector_store
from src.graphrag_anthropic_llamaindex.global_search import SearchModeRouter

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GraphRAGApp:
    def __init__(self):
        # Load environment variables from .env file
        load_dotenv()
        self.config = None
        self.llm_params = {}
        self.vector_stores = {}
        self.is_initialized = False
        self.llm_provider = None
    
    def initialize_config(self, config_path: str = "config/config.yaml") -> str:
        try:
            self.config = load_config(config_path)
            if not self.config:
                return "âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ"
            
            # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®šã‚’å–å¾—
            self.llm_provider = self.config.get("llm_provider", "anthropic")
            
            if self.llm_provider == "bedrock":
                # AWS Bedrockè¨­å®š - ANTHROPIC_API_KEYã¯ä¸è¦
                bedrock_config = self.config.get("bedrock", {})
                model_name = bedrock_config.get("model", "anthropic.claude-3-sonnet-20240229-v1:0")
                region_name = bedrock_config.get("region", "us-east-1")
                aws_access_key_id = bedrock_config.get("aws_access_key_id")
                aws_secret_access_key = bedrock_config.get("aws_secret_access_key")
                aws_session_token = bedrock_config.get("aws_session_token")
                
                # Bedrockç”¨ã®LLMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                self.llm_params = {
                    "model": model_name,
                    "region_name": region_name,
                }
                if aws_access_key_id:
                    self.llm_params["aws_access_key_id"] = aws_access_key_id
                if aws_secret_access_key:
                    self.llm_params["aws_secret_access_key"] = aws_secret_access_key
                if aws_session_token:
                    self.llm_params["aws_session_token"] = aws_session_token
            else:
                # Anthropicç›´æ¥è¨­å®š - API_KEYãŒå¿…è¦
                anthropic_config = self.config.get("anthropic", {})
                # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã®ã¿å–å¾—
                api_key = os.environ.get("ANTHROPIC_API_KEY")
                if not api_key:
                    return "âŒ ANTHROPIC_API_KEY ãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\nexport ANTHROPIC_API_KEY='your-api-key' ã§è¨­å®šã™ã‚‹ã‹ã€\nAWS Bedrock ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ config/config.yaml ã§ llm_provider: 'bedrock' ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
                    
                model_name = anthropic_config.get("model", "claude-3-opus-20240229")
                api_base_url = anthropic_config.get("api_base_url")
                
                self.llm_params = {"model": model_name}
                if api_base_url:
                    self.llm_params["api_base_url"] = api_base_url
            
            # Initialize vector stores
            self.vector_stores = {
                "main": get_vector_store(self.config, store_type="main"),
                "entity": get_vector_store(self.config, store_type="entity"),
                "community": get_vector_store(self.config, store_type="community")
            }
            
            # Configure embedding model
            embedding_config = self.config.get("embedding_model", {})
            embed_model_name = embedding_config.get("name", "intfloat/multilingual-e5-small")
            embed_model = HuggingFaceEmbedding(model_name=embed_model_name)
            
            # Configure chunking
            chunking_config = self.config.get("chunking", {})
            chunk_size = chunking_config.get("chunk_size", 1024)
            chunk_overlap = chunking_config.get("chunk_overlap", 20)
            node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
            
            # Configure Settings based on provider
            if self.llm_provider == "bedrock":
                llm = Bedrock(**self.llm_params)
                Settings.llm = llm
                logger.info(f"Using AWS Bedrock with model: {model_name}")
            else:
                llm = Anthropic(**self.llm_params)
                Settings.llm = llm
                logger.info(f"Using Anthropic API with model: {model_name}")
            
            Settings.embed_model = embed_model
            Settings.node_parser = node_parser
            
            self.is_initialized = True
            logger.info("Configuration initialized successfully")
            return f"âœ… è¨­å®šãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ\nLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.llm_provider}\nãƒ¢ãƒ‡ãƒ«: {model_name}"
            
        except Exception as e:
            error_msg = f"âŒ è¨­å®šã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    
    def search_chat(self, message: str, history: list, search_mode: str, response_type: str, output_format: str, min_community_rank: int, progress=gr.Progress()) -> tuple:
        if not self.is_initialized:
            error_msg = "âŒ è¨­å®šãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã¾ãšè¨­å®šã‚¿ãƒ–ã§è¨­å®šã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚"
            history.append([message, error_msg])
            return "", history
        
        if not message.strip():
            error_msg = "âŒ æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
            history.append([message, error_msg])
            return "", history
        
        try:
            progress(0, desc="æ¤œç´¢ã‚’é–‹å§‹...")
            logger.info(f"Searching: query='{message}', search_mode={search_mode}, response_type={response_type}")
            
            progress(0.5, desc="æ¤œç´¢ä¸­...")
            
            # Use the new SearchModeRouter for unified search interface
            router = SearchModeRouter(
                config=self.config,
                mode=search_mode,
                vector_store_main=self.vector_stores.get("main"),
                vector_store_entity=self.vector_stores.get("entity"),
                vector_store_community=self.vector_stores.get("community"),
                response_type=response_type,
                min_community_rank=min_community_rank,
                output_format=output_format
            )
            
            # Execute search
            query_bundle = QueryBundle(query_str=message)
            results = router._retrieve(query_bundle)
            
            progress(1.0, desc="å®Œäº†")
            logger.info("Search completed successfully")
            
            # Format results
            if results:
                if output_format == "json":
                    # JSONå½¢å¼ã§çµæœã‚’æ•´å½¢
                    result_data = []
                    for i, node_with_score in enumerate(results):
                        result_data.append({
                            "score": node_with_score.score,
                            "text": node_with_score.node.text,
                            "metadata": node_with_score.node.metadata
                        })
                    response = f"ğŸ” **æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰**: {self._get_search_mode_name(search_mode)}\n\n```json\n{json.dumps(result_data, ensure_ascii=False, indent=2)}\n```"
                else:
                    # Markdownå½¢å¼ã§çµæœã‚’æ•´å½¢
                    main_result = results[0].node.text if results else "çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                    response = f"ğŸ” **æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰**: {self._get_search_mode_name(search_mode)}\n\n{main_result}"
                    
                    # è¿½åŠ ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆ
                    if len(results) > 1:
                        response += "\n\n### ğŸ“Œ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ\n"
                        for i, node_with_score in enumerate(results[1:], 1):
                            response += f"\n**ãƒã‚¤ãƒ³ãƒˆ {i}** (ã‚¹ã‚³ã‚¢: {node_with_score.score:.2f})\n{node_with_score.node.text}\n"
            else:
                response = f"ğŸ” **æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰**: {self._get_search_mode_name(search_mode)}\n\nçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            
            history.append([message, response])
            return "", history
            
        except Exception as e:
            error_msg = f"âŒ æ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
            logger.error(error_msg, exc_info=True)
            history.append([message, error_msg])
            return "", history
    
    def _get_search_mode_name(self, search_mode: str) -> str:
        mode_names = {
            "local": "ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ï¼ˆè©³ç´°ãƒ»é«˜ç²¾åº¦ï¼‰",
            "global": "ã‚°ãƒ­ãƒ¼ãƒãƒ«æ¤œç´¢ï¼ˆåŒ…æ‹¬çš„ãƒ»è¦ç´„ï¼‰",
            "drift": "DRIFTæ¤œç´¢ï¼ˆæ¬¡ä¸–ä»£ãƒ»å®Ÿé¨“çš„ï¼‰",
            "auto": "è‡ªå‹•é¸æŠï¼ˆã‚¯ã‚¨ãƒªã«æœ€é©ãªæ–¹æ³•ã‚’è‡ªå‹•é¸æŠï¼‰"
        }
        return mode_names.get(search_mode, search_mode)

# Global app instance
app = GraphRAGApp()

def create_interface():
    with gr.Blocks(title="GraphRAG Anthropic LlamaIndex", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ”— GraphRAG Anthropic LlamaIndex Web App")
        gr.Markdown("CLIã§ä½œæˆã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œç´¢ã™ã‚‹ãŸã‚ã®Webã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹")
        
        with gr.Tab("âš™ï¸ è¨­å®š"):
            gr.Markdown("### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿")
            config_path = gr.Textbox(
                label="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹",
                value="config/config.yaml",
                placeholder="config/config.yamlãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å…¥åŠ›"
            )
            config_btn = gr.Button("è¨­å®šã‚’èª­ã¿è¾¼ã¿", variant="primary")
            config_status = gr.Textbox(
                label="è¨­å®šçŠ¶æ³",
                interactive=False,
                lines=5
            )
            
            config_btn.click(
                fn=app.initialize_config,
                inputs=[config_path],
                outputs=[config_status]
            )
        
        with gr.Tab("ğŸ” æ¤œç´¢"):
            gr.Markdown("### ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§æ¤œç´¢")
            
            with gr.Row():
                with gr.Column(scale=2):
                    search_mode = gr.Dropdown(
                        label="æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰",
                        choices=[
                            ("ã‚°ãƒ­ãƒ¼ãƒãƒ«æ¤œç´¢ï¼ˆåŒ…æ‹¬çš„ï¼‰", "global"),
                            ("ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ï¼ˆè©³ç´°ï¼‰", "local"), 
                            ("DRIFTæ¤œç´¢ï¼ˆå®Ÿé¨“çš„ï¼‰", "drift"),
                            ("è‡ªå‹•é¸æŠ", "auto")
                        ],
                        value="global",
                        info="æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„"
                    )
                with gr.Column(scale=2):
                    response_type = gr.Dropdown(
                        label="å›ç­”ã‚¿ã‚¤ãƒ—",
                        choices=[
                            ("è¤‡æ•°æ®µè½", "multiple paragraphs"),
                            ("å˜ä¸€æ®µè½", "single paragraph"),
                            ("ãƒªã‚¹ãƒˆå½¢å¼", "list"),
                            ("è¦ç‚¹ã®ã¿", "key points")
                        ],
                        value="multiple paragraphs",
                        info="å›ç­”ã®å½¢å¼ã‚’é¸æŠ"
                    )
            
            with gr.Row():
                with gr.Column(scale=2):
                    output_format = gr.Radio(
                        label="å‡ºåŠ›å½¢å¼",
                        choices=[("Markdown", "markdown"), ("JSON", "json")],
                        value="markdown",
                        info="çµæœã®è¡¨ç¤ºå½¢å¼"
                    )
                with gr.Column(scale=2):
                    min_community_rank = gr.Slider(
                        label="æœ€å°ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ©ãƒ³ã‚¯",
                        minimum=0,
                        maximum=5,
                        value=0,
                        step=1,
                        info="0 = ã™ã¹ã¦ã®ãƒ¬ãƒ™ãƒ«"
                    )
            
            # Chat interface
            chatbot = gr.Chatbot(
                value=[],
                label="æ¤œç´¢ãƒãƒ£ãƒƒãƒˆ",
                show_label=True,
                height=400,
                show_copy_button=True
            )
            
            with gr.Row():
                msg = gr.Textbox(
                    label="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
                    placeholder="è³ªå•ã‚„æ¤œç´¢ã—ãŸã„å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
                    lines=2,
                    scale=4
                )
                send_btn = gr.Button("é€ä¿¡", variant="primary", scale=1)
            
            # Clear chat button
            clear_btn = gr.Button("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢", variant="secondary")
            
            # Event handlers
            def send_message(message, history, mode, response_type, output_format, min_rank):
                return app.search_chat(message, history, mode, response_type, output_format, min_rank)
            
            def clear_chat():
                return []
            
            # Send message on button click or Enter
            send_btn.click(
                fn=send_message,
                inputs=[msg, chatbot, search_mode, response_type, output_format, min_community_rank],
                outputs=[msg, chatbot]
            )
            
            msg.submit(
                fn=send_message,
                inputs=[msg, chatbot, search_mode, response_type, output_format, min_community_rank],
                outputs=[msg, chatbot]
            )
            
            # Clear chat
            clear_btn.click(
                fn=clear_chat,
                outputs=[chatbot]
            )
        
        with gr.Tab("â„¹ï¸ ä½¿ç”¨æ–¹æ³•"):
            gr.Markdown("""
            ## ğŸ“– ä½¿ç”¨æ‰‹é †
            
            ### 1ï¸âƒ£ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆï¼ˆCLIã§å®Ÿè¡Œï¼‰
            ```bash
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
            python -m graphrag_anthropic_llamaindex add
            ```
            
            ### 2ï¸âƒ£ Webã‚¢ãƒ—ãƒªã§æ¤œç´¢
            1. **è¨­å®šã‚¿ãƒ–**: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆconfig.yamlï¼‰ã‚’èª­ã¿è¾¼ã‚€
            2. **æ¤œç´¢ã‚¿ãƒ–**: ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰æ¤œç´¢ã‚’å®Ÿè¡Œ
            
            ## ğŸ” æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®èª¬æ˜
            
            - **ã‚°ãƒ­ãƒ¼ãƒãƒ«æ¤œç´¢**: åŒ…æ‹¬çš„ãªè¦ç´„ã¨æ´å¯Ÿã‚’æä¾›ï¼ˆæ¨å¥¨ï¼‰
            - **ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢**: ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«åŸºã¥ã„ãŸè©³ç´°ãªæ¤œç´¢
            - **DRIFTæ¤œç´¢**: æ¬¡ä¸–ä»£ã®å®Ÿé¨“çš„ãªæ¤œç´¢æ–¹æ³•
            - **è‡ªå‹•é¸æŠ**: ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦æœ€é©ãªæ–¹æ³•ã‚’è‡ªå‹•é¸æŠ
            
            ## ğŸ“ å›ç­”ã‚¿ã‚¤ãƒ—
            
            - **è¤‡æ•°æ®µè½**: è©³ç´°ãªèª¬æ˜ã‚’å«ã‚€åŒ…æ‹¬çš„ãªå›ç­”
            - **å˜ä¸€æ®µè½**: ç°¡æ½”ã«ã¾ã¨ã‚ã‚‰ã‚ŒãŸå›ç­”
            - **ãƒªã‚¹ãƒˆå½¢å¼**: ç®‡æ¡æ›¸ãã§æ•´ç†ã•ã‚ŒãŸå›ç­”
            - **è¦ç‚¹ã®ã¿**: é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã®ã¿ã‚’æŠ½å‡º
            
            ## âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¾‹
            
            ```yaml
            # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
            llm_provider: "anthropic"  # ã¾ãŸã¯ "bedrock"
            
            # Anthropic APIè¨­å®š
            anthropic:
              model: "claude-3-opus-20240229"
              # api_keyã¯ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã‹ã‚‰èª­ã¿è¾¼ã¿
            
            # ã¾ãŸã¯ AWS Bedrockè¨­å®š
            bedrock:
              model: "anthropic.claude-3-sonnet-20240229-v1:0"
              region: "us-east-1"
            
            # å…¥å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            input_dir: "./data"
            output_dir: "./graphrag_output"
            
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
            embedding_model:
              name: "intfloat/multilingual-e5-small"
            
            # ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
            chunking:
              chunk_size: 1024
              chunk_overlap: 20
            ```
            
            ## ğŸ’¬ ãƒãƒ£ãƒƒãƒˆæ¤œç´¢ã®ä½¿ã„æ–¹
            
            1. æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠï¼ˆé€šå¸¸ã¯ã€Œã‚°ãƒ­ãƒ¼ãƒãƒ«æ¤œç´¢ã€ãŒãŠã™ã™ã‚ï¼‰
            2. å›ç­”ã‚¿ã‚¤ãƒ—ã¨å‡ºåŠ›å½¢å¼ã‚’é¸æŠ
            3. ãƒãƒ£ãƒƒãƒˆæ¬„ã«è‡ªç„¶ãªæ—¥æœ¬èªã§è³ªå•ã‚’å…¥åŠ›
            4. Enterã‚­ãƒ¼ã¾ãŸã¯ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã§æ¤œç´¢å®Ÿè¡Œ
            5. çµæœãŒãƒãƒ£ãƒƒãƒˆå½¢å¼ã§è¡¨ç¤ºã•ã‚Œã¾ã™
            6. ç¶šã‘ã¦è¿½åŠ ã®è³ªå•ã‚‚å¯èƒ½ã§ã™
            
            ## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
            
            - **API Key ã‚¨ãƒ©ãƒ¼**: ç’°å¢ƒå¤‰æ•° `ANTHROPIC_API_KEY` ã‚’è¨­å®šã™ã‚‹ã‹ã€AWS Bedrockã‚’ä½¿ç”¨
            - **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„**: CLIã§ `python -m graphrag_anthropic_llamaindex add` ã‚’å®Ÿè¡Œ
            - **æ¤œç´¢çµæœãŒç©º**: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦å†è©¦è¡Œ
            """)
    
    return interface

def main():
    interface = create_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )

if __name__ == "__main__":
    main()